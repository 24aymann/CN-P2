#!/bin/bash
# DeclaraciÃ³n de variables de entorno
AWS_REGION="us-east-1"
ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)
BUCKET_NAME="datalake-laureates-${ACCOUNT_ID}"
ROLE_ARN=$(aws iam get-role --role-name LabRole --query 'Role.Arn' --output text)

echo "=========== ğŸ”¹ CONFIGURACIÃ“N INICIAL ğŸ”¹ ==========="
echo "1ï¸âƒ£  RegiÃ³n de AWS:     $AWS_REGION"
echo "2ï¸âƒ£  Bucket:            $BUCKET_NAME"
echo "3ï¸âƒ£  Role:              $ROLE_ARN"

add_blankspaces() {
    echo ""
    echo "==========================================="
    echo ""
}

execute_command() {
    local informative_message="$1"
    local success_message="$2"
    local error_message="$3"
    shift 3
    
    add_blankspaces
    echo "$informative_message"
    "$@" > /dev/null 2>&1

    if [ $? -eq 0 ]; then
        echo "âœ… $success_message"
    else
        echo "âš ï¸ $error_message"
        exit 1
    fi
}


# =======================================
# ======== SECCIÃ“N: Kinesis & S3 ========
# =======================================

# Crear el bucket de S3
execute_command "Creando el bucket de S3 $BUCKET_NAME..." \
                "Â¡Bucket de S3 creado correctamente!" \
                "AVISO: Ha ocurrido un error al crear el bucket de S3." \
                aws s3 mb s3://$BUCKET_NAME

# Crear carpetas del bucket (objetos vacÃ­os con / al final)
execute_command "Creando estructura de carpetas en S3..." \
                "Â¡Estructura de carpetas creada correctamente!" \
                "AVISO: Ha ocurrido un error al crear la estructura de carpetas." \
                bash -c "aws s3api put-object --bucket $BUCKET_NAME --key raw/ && \
                 aws s3api put-object --bucket $BUCKET_NAME --key raw/laureates/ && \
                 aws s3api put-object --bucket $BUCKET_NAME --key processed/ && \
                 aws s3api put-object --bucket $BUCKET_NAME --key config/ && \
                 aws s3api put-object --bucket $BUCKET_NAME --key scripts/ && \
                 aws s3api put-object --bucket $BUCKET_NAME --key queries/ && \
                 aws s3api put-object --bucket $BUCKET_NAME --key errors/"

# Crear el stream de Kinesis
execute_command "Creando el stream de Kinesis: laureates-stream..." \
                "Â¡Stream de Kinesis creado correctamente!" \
                "AVISO: Ha ocurrido un error al crear el stream de Kinesis." \
                aws kinesis create-stream --stream-name laureates-stream --shard-count 1


# =======================================
# ========== SECCIÃ“N: FIREHOSE ==========
# =======================================

# Crear el zip de la lambda
execute_command "Empaquetando la funciÃ³n Lambda..." \
                "Â¡Paquete creado correctamente!" \
                "AVISO: Ha ocurrido un error al empaquetar la funciÃ³n Lambda." \
                python -c "import zipfile, sys; z=zipfile.ZipFile('firehose.zip', 'w'); z.write(sys.argv[1]); z.close()" "firehose.py"

# Crear la funciÃ³n lambda
execute_command "Creando la funciÃ³n Lambda: laureates-firehose-lambda..." \
                "Â¡FunciÃ³n Lambda creada correctamente!" \
                "AVISO: Ha ocurrido un error al crear la funciÃ³n Lambda." \
                aws lambda create-function \
                    --function-name laureates-firehose-lambda \
                    --runtime python3.12 \
                    --role "$ROLE_ARN" \
                    --handler firehose.lambda_handler \
                    --zip-file fileb://firehose.zip \
                    --timeout 60 \
                    --memory-size 128

# Actualizar la funciÃ³n lambda
execute_command "Actualizando el cÃ³digo de la funciÃ³n Lambda..." \
                "Â¡CÃ³digo de Lambda actualizado correctamente!" \
                "AVISO: Ha ocurrido un error al actualizar el cÃ³digo de Lambda." \
                aws lambda update-function-code \
                    --function-name laureates-firehose-lambda \
                    --zip-file fileb://firehose.zip

# Obtener el ARN de la funciÃ³n lambda
LAMBDA_ARN=$(aws lambda get-function --function-name laureates-firehose-lambda --query 'Configuration.FunctionArn' --output text)
if [ -z "$LAMBDA_ARN" ]; then
    echo "AVISO: Ha ocurrido un error al obtener el ARN de la funciÃ³n Lambda."
    exit 1
fi

# Crear el delivery stream
execute_command "Creando Firehose Delivery Stream..." \
                "Â¡Firehose Delivery Stream creado correctamente!" \
                "AVISO: Ha ocurrido un error al crear el Firehose Delivery Stream." \
                aws firehose create-delivery-stream \
                    --delivery-stream-name laureates-delivery-stream \
                    --delivery-stream-type KinesisStreamAsSource \
                    --kinesis-stream-source-configuration "KinesisStreamARN=arn:aws:kinesis:$AWS_REGION:$ACCOUNT_ID"":stream/laureates-stream,RoleARN=$ROLE_ARN" \
                    --extended-s3-destination-configuration "{
                \"BucketARN\": \"arn:aws:s3:::$BUCKET_NAME\",
                \"RoleARN\": \"$ROLE_ARN\",
                \"Prefix\": \"raw/laureates/processing_date=!{partitionKeyFromLambda:processing_date}/\",
                \"ErrorOutputPrefix\": \"errors/!{firehose:error-output-type}/\",
                \"BufferingHints\": {
                    \"SizeInMBs\": 64,
                    \"IntervalInSeconds\": 60
                },
                \"DynamicPartitioningConfiguration\": {
                    \"Enabled\": true,
                    \"RetryOptions\": {
                        \"DurationInSeconds\": 300
                    }
                },
                \"ProcessingConfiguration\": {
                    \"Enabled\": true,
                    \"Processors\": [
                        {
                            \"Type\": \"Lambda\",
                            \"Parameters\": [
                                {
                                    \"ParameterName\": \"LambdaArn\",
                                    \"ParameterValue\": \"$LAMBDA_ARN\"
                                },
                                {
                                    \"ParameterName\": \"BufferSizeInMBs\",
                                    \"ParameterValue\": \"1\"
                                },
                                {
                                    \"ParameterName\": \"BufferIntervalInSeconds\",
                                    \"ParameterValue\": \"60\"
                                }
                            ]
                        }
                    ]
                }
            }"


# =======================================
# ============ SECCIÃ“N: GLUE ============
# =======================================

# Crear la base de datos de Glue
execute_command "Creando la base de datos de Glue: laureates_db..." \
                "Â¡Base de datos de Glue creada correctamente!" \
                "AVISO: Ha ocurrido un error al crear la base de datos de Glue." \
                aws glue create-database --database-input "{\"Name\":\"laureates_db\"}"

# Crear el crawler de Glue
execute_command "Creando el crawler Glue: laureates-raw-crawler..." \
                "Â¡Crawler Glue creado correctamente!" \
                "AVISO: Ha ocurrido un error al crear el crawler Glue." \
                aws glue create-crawler \
                    --name laureates-raw-crawler \
                    --role "$ROLE_ARN" \
                    --database-name laureates_db \
                    --targets "{\"S3Targets\": [{\"Path\": \"s3://$BUCKET_NAME/raw/laureates\"}]}"

add_blankspaces
echo "Ejecutando el producer de Kinesis (tardarÃ¡ unos momentos)..."
python kinesis.py
if [[ $? -eq 0 ]]; then
    echo "âœ… Â¡Datos enviados a Kinesis!"
else
    echo "âš ï¸ AVISO: Ha ocurrido un error al ejecutar el producer de Kinesis."
    exit 1
fi
sleep 60

execute_command "Iniciando el crawler 'laureates-raw-crawler'..." \
                "Â¡Crawler iniciado!" \
                "AVISO: Ha ocurrido un error al iniciar el crawler." \
                aws glue start-crawler --name laureates-raw-crawler


# =======================================
# ========== SECCIÃ“N: GLUE ETL ==========
# =======================================

# Subir los scripts de ETL a S3
execute_command "Subiendo scripts ETL a S3..." \
                "Â¡Scripts ETL subidos correctamente!" \
                "AVISO: Ha ocurrido un error al subir scripts ETL." \
                bash -c "aws s3 cp jobs/nobel_aggregation_gender.py s3://$BUCKET_NAME/scripts/ && \
                 aws s3 cp jobs/nobel_aggregation_decadal.py s3://$BUCKET_NAME/scripts/ && \
                 aws s3 cp jobs/nobel_aggregation_by_country.py s3://$BUCKET_NAME/scripts/"

# Variables de entorno
DATABASE="laureates_db"
TABLE="laureates"
GENDER_OUTPUT="s3://$BUCKET_NAME/processed/laureates_gender/"
DECADAL_OUTPUT="s3://$BUCKET_NAME/processed/laureates_decadal/"
COUNTRY_OUTPUT="s3://$BUCKET_NAME/processed/laureates_country/"

# CreaciÃ³n de los Jobs de Glue
execute_command "Â¡Creando primer Job Glue! ---> 1ï¸âƒ£  Nobel Gender Aggregation..." \
                "Â¡Job nobel-gender-aggregation creado correctamente!" \
                "AVISO: Ha ocurrido un error al crear el Job nobel-gender-aggregation." \
                aws glue create-job \
                    --name nobel-gender-aggregation \
                    --role "$ROLE_ARN" \
                    --command "{
                        \"Name\": \"glueetl\",
                        \"ScriptLocation\": \"s3://$BUCKET_NAME/scripts/nobel_aggregation_gender.py\",
                        \"PythonVersion\": \"3\"
                    }" \
                    --default-arguments "{
                        \"--database\": \"$DATABASE\",
                        \"--table\": \"$TABLE\",
                        \"--output_path\": \"s3://$BUCKET_NAME/processed/laureates_by_gender/\",
                        \"--enable-continuous-cloudwatch-log\": \"true\",
                        \"--spark-event-logs-path\": \"s3://$BUCKET_NAME/logs/\"
                    }" \
                    --glue-version "4.0" \
                    --number-of-workers 2 \
                    --worker-type "G.1X"

execute_command "Â¡Creando segundo Job Glue! ---> 2ï¸âƒ£  Nobel Decadal Aggregation..." \
                "Â¡Job nobel-decadal-aggregation creado correctamente!" \
                "AVISO: Ha ocurrido un error al crear el Job nobel-decadal-aggregation." \
                aws glue create-job \
                    --name nobel-decadal-aggregation \
                    --role "$ROLE_ARN" \
                    --command "{
                        \"Name\": \"glueetl\",
                        \"ScriptLocation\": \"s3://$BUCKET_NAME/scripts/nobel_aggregation_decadal.py\",
                        \"PythonVersion\": \"3\"
                    }" \
                    --default-arguments "{
                        \"--database\": \"$DATABASE\",
                        \"--table\": \"$TABLE\",
                        \"--output_path\": \"s3://$BUCKET_NAME/processed/laureates_by_decadal/\",
                        \"--enable-continuous-cloudwatch-log\": \"true\",
                        \"--spark-event-logs-path\": \"s3://$BUCKET_NAME/logs/\"
                    }" \
                    --glue-version "4.0" \
                    --number-of-workers 2 \
                    --worker-type "G.1X"

execute_command "Â¡Creando tercer Job Glue! ---> 3ï¸âƒ£  Nobel Country Aggregation..." \
                "Â¡Job nobel-country-aggregation creado correctamente!" \
                "AVISO: Ha ocurrido un error al crear el Job nobel-country-aggregation." \
                aws glue create-job \
                    --name nobel-country-aggregation \
                    --role "$ROLE_ARN" \
                    --command "{
                        \"Name\": \"glueetl\",
                        \"ScriptLocation\": \"s3://$BUCKET_NAME/scripts/nobel_aggregation_by_country.py\",
                        \"PythonVersion\": \"3\"
                    }" \
                    --default-arguments "{
                        \"--database\": \"$DATABASE\",
                        \"--table\": \"$TABLE\",
                        \"--output_path\": \"s3://$BUCKET_NAME/processed/laureates_by_country/\",
                        \"--enable-continuous-cloudwatch-log\": \"true\",
                        \"--spark-event-logs-path\": \"s3://$BUCKET_NAME/logs/\"
                    }" \
                    --glue-version "4.0" \
                    --number-of-workers 2 \
                    --worker-type "G.1X"

add_blankspaces
echo "Esperando a que el crawler 'laureates-raw-crawler' termine..."
while true; do
    CRAWLER_STATE=$(aws glue get-crawler --name laureates-raw-crawler --query "Crawler.State" --output text)
    echo "Estado del Crawler: $CRAWLER_STATE"
    
    if [ "$CRAWLER_STATE" == "READY" ]; then
        echo ""
        echo "Crawler finalizado."
        break
    fi
    
    sleep 30
done

# Comenzar la ejecuciÃ³n de los Jobs
execute_command "Iniciando Jobs de Glue..." \
                "Â¡Jobs iniciados correctamente!" \
                "AVISO: Ha ocurrido un error al iniciar los Jobs de Glue." \
                bash -c "aws glue start-job-run --job-name nobel-gender-aggregation && \
                 aws glue start-job-run --job-name nobel-decadal-aggregation && \
                 aws glue start-job-run --job-name nobel-country-aggregation"

# Comprobar el estado de los Jobs
aws glue get-job-runs --job-name nobel-gender-aggregation --max-items 1 > /dev/null
aws glue get-job-runs --job-name nobel-decadal-aggregation --max-items 1 > /dev/null
aws glue get-job-runs --job-name nobel-country-aggregation --max-items 1 > /dev/null

add_blankspaces
echo "ğŸ‰  F I N   D E L   D E S P L I E G U E  ğŸ‰"
echo ""
