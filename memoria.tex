\documentclass[a4paper,12pt]{article}

% Paquetes de configuración
\usepackage[utf8]{inputenc}
\usepackage[spanish]{babel}
\usepackage[T1]{fontenc}
\usepackage{mathptmx} % Fuente times new roman
\usepackage{helvet}   % Fuente arial
\usepackage{geometry}
\usepackage{setspace}
\usepackage{fancyhdr}
\usepackage{graphicx}
\usepackage{titlesec}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{hyperref}
\usepackage{caption}
\usepackage{parskip}
\usepackage{ragged2e} % Librería para texto justificado

% Configuración de página
\geometry{
    top=2.5cm,
    bottom=2.5cm,
    left=3cm,
    right=2.5cm
}
\onehalfspacing
\pagestyle{fancy}
\fancyhf{}
\fancyhead[R]{Computación en la Nube - Práctica Obligatoria 2}
\fancyfoot[C]{Página \thepage\ de \pageref{LastPage}}
\renewcommand{\headrulewidth}{0.4pt}
\renewcommand{\footrulewidth}{0pt}

% Conteo del total de páginas
\usepackage{lastpage}
% Configuración de fuentes para títulos
\titleformat{\section}
    {\normalfont\sffamily\bfseries\fontsize{14}{17}\selectfont}
    {\thesection}{1em}{}
\titleformat{\subsection}
    {\normalfont\sffamily\bfseries\fontsize{13}{16}\selectfont}
    {\thesubsection}{0.8em}{}
\titleformat{\subsubsection}
    {\normalfont\sffamily\bfseries\fontsize{12}{15}\selectfont}{\thesubsubsection}{0.8em}{}

% Espaciado de párrafos
\titlespacing*{\section}{0pt}{12pt}{0pt}
\titlespacing*{\subsection}{0pt}{12pt}{0pt}
\titlespacing*{\subsubsection}{0pt}{12pt}{0pt}

% Configuración de bloques de código
\definecolor{codegray}{rgb}{0.96,0.96,0.96}
\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.96,0.96,0.96}

\lstdefinestyle{mystyle}{
    backgroundcolor=\color{backcolour},   
    commentstyle=\color{codegreen},
    keywordstyle=\color{magenta},
    numberstyle=\tiny\color{codegray},
    stringstyle=\color{codepurple},
    basicstyle=\ttfamily\footnotesize,
    breakatwhitespace=false,         
    breaklines=true,                 
    captionpos=b,                    
    keepspaces=true,                 
    numbers=none,                    
    numbersep=5pt,                  
    showspaces=false,                
    showstringspaces=false,
    showtabs=false,                  
    tabsize=2,
    frame=single,
    rulecolor=\color{lightgray}
}

\lstset{style=mystyle}

\begin{document}

% -----------------------
% ------- PORTADA -------
% -----------------------

\begin{titlepage}
    \centering
    \vspace*{2cm}
    
    {\Large \textbf{UNIVERSIDAD DE LAS PALMAS DE GRAN CANARIA}} \\
    \vspace{0.5cm}
    {\large GRADO EN INGENIERÍA INFORMÁTICA} \\
    
    \vspace{2.5cm}
    
    {\Large \textbf{COMPUTACIÓN EN LA NUBE - PRÁCTICA OBLIGATORIA 2}} \\
    \vspace{0.5cm}
    {\Large \textbf{INGESTA Y PROCESAMIENTO DE DATOS EN AWS}} \\
    
    \vspace{3cm}
    \begin{center}
    \includegraphics[width=7cm]{icon.png}
    \end{center}
    \vspace{4cm}

    
    \raggedleft
    {\large Ayman Asbai Ghoudan} \\   
    {\large Curso académico 2025/26} \\
    
    \vfill
\end{titlepage}
\justifying
\setlength{\parindent}{0pt}

% -----------------------
% -------- ÍNDICE -------
% -----------------------

\newpage
\tableofcontents
\newpage

% -----------------------
% ----- INTRODUCCIÓN ----
% -----------------------

\section{Introducción}

La segunda práctica entregable de la asignatura consiste en el correcto aprendizaje de la manera en la que recibir datos masivos de mano de AWS Kinesis, almacenarlos de forma segura con un bucket de S3 y someterlos a una transformación con AWS Glue para que puedan ser analizados posteriormente.

Para llevar esto a cabo, se ha ido desarrollando una arquitectura de procesamiento de datos escalable y desacoplada por medio de los servicios que se proporcionan en el laboratorio de AWS Academy. Entre otros aspectos, esta práctica contribuye a visualizar la relevancia de acciones como ingerir, almacenar y procesar flujos de datos en tiempo real, fundamental para cualquier organización hoy en día en el ámbito informático.

Además de ello, se ha simulado un escenario de streaming de datos utilizando un conjunto de datos que recoge los ganadores de premios Nobel desde 1901 hasta 2016. El flujo de trabajo implementado cubre el ciclo de vida completo del dato, tal que:

\begin{enumerate}
    \item \textbf{Ingesta de Datos:} Se utiliza un script \texttt{kinesis.py} de Python para simular un productor que inyecta registros en tiempo real hacia \textit{Amazon Kinesis Data Streams}, un servicio altamente escalable para el procesamiento de flujos de datos.
    \item \textbf{Almacenamiento y Transformación Intermedia:} A través de \textit{Amazon Kinesis Firehose}, los datos son recogidos del stream, transformados ligeramente mediante una función de \textit{AWS Lambda} para permitir un particionado dinámico eficiente, y finalmente depositados en un datalake alojado en \textit{Amazon S3}.
    \item \textbf{Catalogación y ETL:} Una vez los datos residen en S3, se utiliza \textit{AWS Glue}. En primer lugar, los \textit{crawlers} de Glue descubren automáticamente el esquema de los datos. En segundo lugar, se ejecutan \textit{jobs ETL} para agregar y analizar la información (uno de los casos utilizados en esta práctica ha sido obtener métricas a raíz del género), depositando los resultados procesados nuevamente en S3 en formato \textit{parquet}.
\end{enumerate}

De este modo, la arquitectura planteada permite separar adecuadamente las responsabilidades y lograr el objetivo de la práctica.

\newpage

% -------------------------------------
% --- DESARROLLO DE LAS ACTIVIDADES ---
% -------------------------------------

\section{Desarrollo de las actividades}

En este apartado de la memoria del trabajo, se detallarán cada uno de los pasos que se han seguido para cumplir con la implementación de cada uno de los componentes de la arquitectura.

% ===================================================
% ===================================================

\subsection{Configuración del bucket S3}

\textit{Amazon Simple Storage Service} es considerado uno de los pilares fundamentales en el mundo de la computación en la nube dada su escalabilidad prácticamente ilimitada, su alta disponibilidad y su durabilidad. Al ser un servicio de almacenamiento de objetos, también conocido como "\textit{el cubo para todo}", permite desacoplar el almacenamiento del cómputo, una característica esencial en los ecosistemas de datos modernos.

Su relevancia radica no sólo en la persistencia de los datos, sino en su capacidad para integrarse de forma nativa con servicios de análisis y procesamiento como AWS Glue y Amazon Athena.

El primer paso para consolidar el mencionado datalake es la creación de un bucket en Amazon S3, el cual servirá como repositorio central tanto para los datos crudos como para los datos procesados y los scripts de configuración.

La particularidad de este servicio de AWS es que debe disponer de un nombre único no sólo en términos de la cuenta de usuario, sino en toda la plataforma. De esta manera, cada bucket puede ser accedido mediante URLs como, por ejemplo:

\vspace{0.25cm}
\centerline{\texttt{https://<nombre-del-bucket>.s3.amazonaws.com}}
\vspace{0.25cm}

Es por ello que se ha seguido este patrón y se ha decidido configurar el mismo con un nombre característico como \textbf{datalake-laureates-<ID\_CUENTA>}, en el que, al incluir el ID de cuenta, se garantiza que no pueda haber ningún conflicto. Dentro de este bucket, se ha definido una estructura de directorios jerárquica para mantener el orden y facilitar las políticas de ciclo de vida y acceso. La estructura implementada es la que se aprecia a continucación:

\begin{itemize}
    \item \textbf{raw/}: Esta carpeta está destinada a almacenar los datos tal cual llegan desde el productor, sin modificaciones analíticas. Dentro de ésta, se utiliza la subcarpeta \texttt{laureates/} para los registros propios de los premios Nobel.
    \item \textbf{processed/}: Contendrá los resultados de los jobs ETL de Glue, almacenados típicamente en formatos columnares, tal como Parquet, para optimizar consultas posteriores.
    \item \textbf{config/} y \textbf{scripts/}: Almacenan ficheros de configuración y los scripts de Python utilizados por los jobs de Glue, asegurando que la infraestructura como código esté centralizada.
    \item \textbf{queries/} y \textbf{errors/}: Son carpetas auxiliares para guardar consultas de Athena o registros de errores generados por Kinesis Firehose si fallase la transformación o entrega.
\end{itemize}

Esta separación de carpetas, la cual puede apreciarse visualmente con la Figura \ref{fig:s3_structure}, es fundamental para evitar mezclar datos de diferentes módulos de procesamiento y para simplificar la configuración de los crawlers de Glue, que pueden apuntar a carpetas específicas sin riesgo de leer archivos de configuración como si fueran datos.

Cabe destacar que no todas estas carpetas tienen uso actualmente, dado que están impuestas con visión a futuro para mejoras e integraciones, tales como puede ser AWS Athena, la cual permite ejecutar consultas/queries directamente sobre datos almacenados en el bucket de S3.

\begin{figure}[h]
    \centering
    \includegraphics[width=0.8\textwidth]{s3.png}
    \caption{Estructura de carpetas creada en el bucket S3.}
    \label{fig:s3_structure}
\end{figure}

\newpage

% ===================================================
% ===================================================

\subsection{Implementación del productor de datos}

El servicio de transmisión de datos del que se hará uso en esta sección es \textbf{Amazon Kinesis Data Streams} y, además de ser masivamente escalable y duradero, está diseñado para capturar y procesar grandes volúmenes de datos en tiempo real. Dentro de la arquitectura del datalake de \textit{laureates}, Kinesis actúa como un búfer, permitiendo desacoplar a los productores de datos de los consumidores.

Su funcionamiento se basa en \textbf{shards}, que se definen como fragmentos, los cuales proporcionan una capacidad de procesamiento dedicada, permitiendo una ingesta ordenada y segura. En este proyecto, el uso de Kinesis es fundamental para transformar una carga de datos estática en un flujo continuo de elementos, simulando un entorno de producción donde la información llega de manera continuada.

Para simular la entrada de datos en tiempo real, se ha desarrollado un script en Python, denominado \texttt{kinesis.py}. Dicho script actúa como un productor que lee un fichero estático JSON (\texttt{nobel\_laureates.json}) y envía cada registro individualmente a un \textit{Amazon Kinesis Data Stream} llamado \texttt{laureates-stream}.

El diseño de este productor de datos incluye las siguientes características clave:
\begin{itemize}
    \item \textbf{Carga de datos:} Se procede a leer el archivo JSON completo que contiene información histórica de todos los premiados (comprendidos entre 1901-2016).
    \item \textbf{Envío continuo:} Recorre la lista de laureados y utiliza la librería \texttt{boto3} para realizar la llamada a \texttt{put\_record}, la cual se encarga de enviar a Kinesis el registro completo (con cada uno de los campos del registro: 'prize', 'full name', entre otros).
    \item \textbf{Clave de partición} Se utiliza el campo \texttt{Category} como clave de partición. Esto asegura que los registros de la misma categoría se dirijan preferentemente al mismo \textit{shard} (fragmento) de Kinesis, lo cual puede ser beneficioso si se quisiera mantener el orden dentro de una categoría. No obstante, en este caso se utiliza un sólo shard para la práctica.
    \item \textbf{Simulación de un periodo de latencia:} También, se ha añadido un \texttt{time.sleep(0.1)} en el script entre envíos para evitar saturar el ancho de banda de subida y simular un flujo continuo de elementos, en lugar de una carga por lotes instantánea.
\end{itemize}

\newpage
A continuación, pese a que el proyecto completo se encuentre adjuntado al final del documento (ver anexo \ref{kinesis}), se muestra un extracto relevante del código del productor donde se realiza el envío del registro:

\begin{lstlisting}[language=Python, caption=Extracto del script del productor de Kinesis (\ref{kinesis})]
# Se itera sobre todos los laureados
for laureate in laureates_list:
    category = laureate.get('Category', 'Unknown')

    # Se envia a Kinesis el registro completo
    kinesis.put_record(
        StreamName=STREAM_NAME, # 'laureates-stream'
        Data=json.dumps(laureate),
        PartitionKey=category
    )
    # ... Log y sleep ...
\end{lstlisting}

Este enfoque permite verificar en tiempo real cómo la métrica de \texttt{IncomingRecords} aumenta en la consola de Kinesis.

% ===================================================
% ===================================================

\subsection{Configuración del consumidor: Kinesis Firehose}

Como consumidor del stream de datos, se ha configurado el recurso Amazon Kinesis Data Firehose. Firehose, es un servicio de entrega totalmente gestionado que, en la presente arquitectura, no hace más que capturar los datos del stream y los deposita en el bucket de S3.

En cuanto a este caso, la configuración del (\texttt{laureates-delivery-stream}) incluye una funcionalidad avanzada y crucial para la organización del datalake: \textit{el particionado dinámico}.

\subsubsection{Transformación con AWS Lambda}
Para lograr este particionado, Firehose invoca una función \texttt{laureates-firehose-lambda}) de AWS Lambda antes de escribir en S3. El código de esta función, alojado en \texttt{firehose.py}, realiza secuencialmente lo siguiente:

\begin{enumerate}
    \item Recibe un lote de registros codificados en Base64.
    \item Decodifica cada registro y calcula la fecha actual de procesamiento.
    \item Añade metadatos al registro de salida, específicamente una clave \texttt{partitionKeys} con el valor de la fecha (\texttt{processing\_date}).
    \item Devuelve el registro a Firehose.
\end{enumerate}

Esta transformación es ligera pero esencial, ya que permite a Firehose saber en qué carpeta exacta de S3 debe colocar cada archivo.

\subsubsection{Configuración del Destino S3}
En la configuración del destino S3 dentro de Firehose, se ha definido el prefijo utilizando la variable extraída por la Lambda, de modo que el prefijo configurado queda de la siguiente forma:

\vspace{0.1cm}
\begin{center}
\texttt{raw/laureates/processing\_date=!{partitionKeyFromLambda:processing\_date}}
\end{center}
\vspace{0.1cm}

Esto resulta en que los archivos se guarden en rutas como \texttt{laureates-}\texttt{firehose-}\texttt{lambda}) automáticamente, por ejemplo. Este esquema de particionado conformado por parejas clave-valor optimiza enormemente el rendimiento de los Crawlers de Glue y las consultas posteriores en AWS Athena, ya que permite podar o deshacerse de particiones innecesarias al leer datos.

Además, tal y como se refleja en el script de despliegue de la arquitectura, se configuraron opciones de \textit{Buffering} (conformadas por 64 MB o 60 segundos) para agrupar múltiples registros pequeños en archivos más grandes, mejorando la eficiencia de la entrada/salida en S3.

% ===================================================
% ===================================================

\subsection{Configuración de AWS Glue}

El componente final que conformará esta memoria es AWS Glue, el cual se encarga de catalogar y procesar los datos almacenados en S3. Su implementación se divide en dos etapas, la del descubrimiento de esquema y la de la transformación de datos. Seguidamente, se exponen dichas etapas de manera respectiva:

\subsubsection{Crawlers y catálogo de datos}
Para este proyecto, se han configurado dos crawlers:
\begin{enumerate}
    \item \textbf{laureates-raw-crawler}: Apunta a la ruta \texttt{s3://.../raw/laureates/}. Su función es leer los archivos JSON generados por Firehose, inferir el esquema (que presenta campos como \textit{id, first\_name, category}, entre otros) y crear una tabla en la base de datos \texttt{laureates\_db} del Glue Data Catalog.

    \item \textbf{laureates-processed-crawler}: A diferencia del anterior, este apunta a la ruta \texttt{processed/}. Este se ejecuta después de los jobs ETL para catalogar las nuevas tablas de datos agregados.
\end{enumerate}

El uso de Crawlers automatiza la gestión de los metadatos, de modo que si el esquema de los datos JSON cambiara en el futuro, el Crawler podría actualizar la definición de la tabla automáticamente en la siguiente ejecución. Un ejemplo de este caso podría ser una situación donde se añadiera un nuevo campo al JSON de origen.

\subsubsection{ETL Jobs}
Se han desarrollado tres jobs de ETL utilizando la librería de PySpark, los cuales aprovechan la capacidad de procesamiento distribuido de Glue. Los scripts que se observan a continuación, leen la tabla catalogada (\texttt{raw}), realizan agregaciones y escriben el resultado en formato Parquet. Cada una de estas acciones son las que conforman el nombre del recurso ETL, asociándose a: Extract, Transform y Load.

\begin{itemize}
    \item \textbf{Agregación por país:} Agrupa los datos por \texttt{birth\_country} y \texttt{category}, contando el número total de premiados en este contexto. El resultado se escribe particionado por país, lo que facilita consultas geográficas, tales como podrían ser las regiones con más personas premiadas. (Anexo \ref{nobel_aggregation_by_country})

    \item \textbf{Agregación por década:} Calcula la década de cada premio a partir del campo \texttt{year}, y agrupa los resultados para mostrar tendencias temporales. (Anexo \ref{nobel_aggregation_decadal})

    \item \textbf{Agregación por género:} Analiza la distribución de premios entre hombres, mujeres y organizaciones, agrupando por \texttt{gender} y \texttt{category}. (Anexo \ref{nobel_aggregation_gender})
\end{itemize}

Todos estos jobs siguen un patrón común y secuencial, que se basa en lo siguiente: 

\begin{enumerate}
    \item Inicializar el \texttt{GlueContext} y crear un \textit{DynamicFrame} desde el catálogo.

    \item Convertir dicho DynamicFrame a un DataFrame de Spark para facilitar las transformaciones, que son el filtrado, "grouping" y conteo.

    \item Escribir el resultado de nuevo a S3 utilizando \texttt{glueContext.write\_dynamic\_frame}.
\end{enumerate}

El uso del formato Parquet con compresión Snappy en la salida garantiza que los datos procesados ocupen menos espacio y, además, sean mucho más rápidos de consultar que los JSON originales.

\vspace{0.5cm}
\begin{lstlisting}[language=Python, caption=Lógica del job by\_country en PySpark (\ref{nobel_aggregation_by_country})]
# Agrupar por pais de nacimiento y cataegoria
aggregated_df = df.filter(col("birth_country") != "") \
    .groupBy("birth_country", "category") \
    .agg(count("*").alias("total_laureates")) \
    .orderBy("birth_country", "category")

# Escritura a S3 en formato Parquet
glueContext.write_dynamic_frame.from_options(
    frame=output_dynamic_frame,
    connection_type="s3",
    connection_options={
        "path": output_path,
        "partitionKeys": ["birth_country"] 
    },
    format="parquet",
    format_options={"compression": "snappy"}
)
\end{lstlisting}
\newpage

% -----------------
% ---- PRICING ----
% -----------------
%\section{Presupuesto y estimación de costes}

% ----------------------
% ---- CONCLUSIONES ----
% ----------------------


% ------------------------------------
% ---- REFERENCIAS Y BIBLIOGRAFÍA ----
% ------------------------------------
\section{Referencias y bibliografía}

En cuanto a las referencias que han sido utilizadas para llevar a cabo el desarrollo de la práctica, además del uso de la IA registrado en el punto \ref{ia}, se encuentran las siguientes:

\begin{itemize}
    \item \href{https://docs.aws.amazon.com/streams/latest/dev/introduction.html}{Amazon Web Services. What is Amazon Kinesis Data Streams?}

    \item \href{https://docs.aws.amazon.com/streams/latest/dev/kdf-consumer.html}{Amazon Web Services. Develop consumers using Amazon Data Firehose}

    \item \href{https://docs.aws.amazon.com/glue/latest/dg/author-glue-job.html}{Amazon Web Services. Working with jobs in AWS Glue}

    \item \href{https://calculator.aws}{Amazon Web Services. AWS Pricing calculator}

\end{itemize}

% ----------------
% ---- ANEXOS ----
% ----------------

\section{Anexos}

A lo largo de la redacción de la presente memoria, se ha detectado que LaTeX genera ciertos problemas con algunos caracteres, como pueden ser las tildes y los emoticonos, impidiendo su correcta presentación. Por tanto, es posible que algunos de los ficheros que se listen a continuación pierdan formato.

No obstante, antes de proceder con los scripts programados, también se adjunta un enlace al repositorio de GitHub en el que se ha estado implementando dicho proyecto, subsanando así cualquier inconveniente o molestia a la hora de revisar el código. También, es importante tener en cuenta que el archivo que conforma el dataset puede ser localizado 

\vspace{0.1cm}
\begin{center}
\texttt{https://github.com/24aymann/CN-P2}
\end{center}
\vspace{0.1cm}

Además de ello, se debe mencionar que los scripts implementados (tanto para el despliegue como la eliminación de los recursos) cubren todo el trabajo necesario para poder ejecutar la práctica. Esto implica que para corregir y testear el funcionamiento de la misma no hace falta más que:

\begin{enumerate}
    \item Ingresar las credenciales de su cuenta de AWS cuando manda a ejecutar el comando \texttt{aws configure}

    \item Seguir las indicaciones que encuentra en el fichero README.md (\ref{readme}) para configurar su entorno virtual, si lo que se desea es lanzar la arquitectura.

    \item Por último, ejecutar el script correspondiente.
\end{enumerate}

Sin más indicaciones que adjuntar, toda la implementación desarrollada en este proyecto se encuentra en la siguiente página, siguiendo el orden de aparición de la carpeta en la que se aloja.  

\newpage
\subsection{ETL Job - Nobel Aggregation by Country (\texttt{nobel\_aggregation\_by\_country.py})}
\label{nobel_aggregation_by_country}
\lstinputlisting[language=Bash, caption=Job 1 - Aggregation by country]{jobs/nobel_aggregation_by_country.py}

\newpage
\subsection{ETL Job - Nobel Aggregation Decadal (\texttt{nobel\_aggregation\_decadal.py})}
\label{nobel_aggregation_decadal}
\lstinputlisting[language=Bash, caption=Job 2 - Aggregation by decade]{jobs/nobel_aggregation_decadal.py}

\newpage
\subsection{ETL Job - Nobel Aggregation Gender (\texttt{nobel\_aggregation\_gender.py})}
\label{nobel_aggregation_gender}
\lstinputlisting[language=Bash, caption=Job 3 - Aggregation by gender]{jobs/nobel_aggregation_gender.py}

\newpage
\subsection{Script de eliminación de la arquitectura (\texttt{delete\_resources.sh})}
\label{delete_script}
\lstinputlisting[language=Bash, caption=Eliminación de recursos de la arquitectura completo]{scripts/delete_resources.sh}

\subsection{Script de despliegue de la arquitectura (\texttt{deploy\_resources.sh})}
\label{deployment_script}
\lstinputlisting[language=Bash, caption=Despliegue de la arquitectura completo]{scripts/deploy_resources.sh}

\newpage
\subsection{Consumidor del stream de datos (\texttt{firehose.py})}
\label{firehose}
\lstinputlisting[language=Bash, caption=Consumidor del stream de datos]{firehose.py}

\newpage
\subsection{Productor de datos (\texttt{kinesis.py})}
\label{kinesis}
\lstinputlisting[language=Bash, caption=Productor de datos]{kinesis.py}

\subsection{Función main (\texttt{main.py})}
\label{main}
\lstinputlisting[language=Bash, caption=El main de la práctica]{main.py}

\subsection{Versión y dependencias necesarias del proyecto (\texttt{pyproject.toml})}
\label{pyproject}
\lstinputlisting[language=Bash, caption=Fichero con versión y dependencias necesarias]{pyproject.toml}

\newpage
\subsection{README elaborado para la preparación del entorno}
\label{readme}
\lstinputlisting[language=Bash, caption=Presentación de la práctica y configuración del entorno]{README.md}

\subsection{Lockfile de las dependencias en uso}
\label{uv}
\lstinputlisting[language=Bash, caption=Fichero que congela todas las dependencias del proyecto]{uv.lock}

\subsection{Uso de la Inteligencia Artificial}
\label{ia}
Este apartado está planteado para definir aquellas facetas de la práctica en las que se haya beneficiado en mayor o menor medida de la Inteligencia Artificial.

En primer lugar, en los comienzos de realización del proyecto, se hizo uso de la misma para una explicación breve del funcionamiento de cada uno de los recursos de AWS que se manifiestan en este documento. Esto se debe a la necesidad de tener referencias teóricas que explicasen los servicios utilizados, puesto que tras la última sesión de teoría, éstos no fueron publicados.

En segundo lugar, los scripts de despliegue y eliminación de recursos de la arquitectura fueron ligeramente apoyados por la IA, especialmente en análisis de estado de los crawlers y jobs de Glue.

Además de ello, el dataset seleccionado para este proyecto surge de una consulta a esta herramienta. En dicha consulta, se adjuntó el conjunto de datos de consumo energético proporcionado en el proyecto base y se solicitó un dataset con condiciones similares, respetando el límite de tener más de 864 registros.

Por último, también se solicitó formatear la salida que se genera en la terminal con los registros que han sido enviados a Kinesis. Dicho formateo se presenta de la siguiente manera:

\vspace{0.5cm}
\begin{lstlisting}[language=Python, caption=Mensaje de log generado en el envío de datos]
# Indicador de registro enviado, con salida formateada
logger.info(f"( {year} - {category} )".ljust(23) + f"===>  �� {laureate_name} ��")
\end{lstlisting}

\end{document}
